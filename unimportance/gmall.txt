
springboot项目，gmail谷粒商城

项目结构：
gamll-parent 
所有model的根模块，负责管理版本号，定义好各个框架的版本，maven管理

gamll-api
提供持久层和bean的支持，pom文件中依赖了mybatis，java代码中写了bean类和service接口

gamil-user 用户相关


三个util
gamll-commons-util 一般工具模块，给所有的模块使用的工具模块
gamll-web-util 前端专用工具模块，给web模块（控制层）使用的工具模块
gamll-service-util 后端专用工具模块，给service模块（业务层）使用的工具模块


模块开始分层：
web后缀是对外接口，对外提供数据
service后缀是对内，读取数据库，为web提供数据


web和service的项目模块
gamil-user-web, gamil-user-service 用户相关
gmall-manage-web, gmall-manage-serivce 为前端提供数据
gmall-item-web 前台服务(不再需要gmall-item-serivce了，直接调用其他service模块即可)




SOA面向服务
多个微服务组成服务集群，所以要把serivce模块拆分出来
每个业务对应一个微服务，如果有新的功能模块，只需要把这些微服务组合一下就可以对外提供



缓存、分布式锁，高并发压力问题

--------------------


$ ssh-keygen -t rsa -C "wowuruixiong@163.com"
Generating public/private rsa key pair.
Enter file in which to save the key (/c/Users/wuruixiong/.ssh/id_rsa):
Created directory '/c/Users/wuruixiong/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /c/Users/wuruixiong/.ssh/id_rsa.
Your public key has been saved in /c/Users/wuruixiong/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:Iv89sMB6Jp2aAaurn3MsfwYMvEbC1yaGa/jbXvKu8Iw wowuruixiong@163.com
The key's randomart image is:
+---[RSA 2048]----+
|                 |
|                 |
|... .            |
|.o++ o           |
|.o=++.. S        |
|.oooooo..        |
|.oo.o+oo o       |
| .+B=*B....      |
|=oEBXXo . ..     |
+----[SHA256]-----+

hw08603606
hw08603606/hw08603606
--------------------

gmall的数据结构表，实战项目用过的表，实际项目可以更加复杂：

| oms_cart_item               |
| oms_company_address         |
| oms_order                   |
| oms_order_item              |
| payment_info                |
| pms_base_attr_info          |
| pms_base_attr_value         |
| pms_base_catalog1           |
| pms_base_catalog2           |
| pms_base_catalog3           |
| pms_base_sale_attr          |
| pms_brand                   |
| pms_comment                 |
| pms_comment_replay          |
| pms_product_image           |
| pms_product_info            |
| pms_product_sale_attr       |
| pms_product_sale_attr_value |
| pms_product_vertify_record  |
| pms_sku_attr_value          |
| pms_sku_image               |
| pms_sku_info                |
| pms_sku_sale_attr_value     |
| ums_member                  |
| ums_member_level            |
| ums_member_receive_address  |
| wms_ware_info               |
| wms_ware_order_task         |
| wms_ware_order_task_detail  |
| wms_ware_sku                |


表关系：
pms_base_catalog1 分类一号列表，大分类，可以看做是首页的总分类
pms_base_catalog2 分类二号列表，属于一号分类列表的子项，每一行都要对应一个一号分类的id，这个id相当于外键
pms_base_catalog3 分类三号列表，属于二号分类列表的子项，每一行都要对应一个二号分类的id，这个id相当于外键


三号分类列表衍生出spu
pms_product_info  spu，SPU = Standard Product Unit （标准化产品单元）
pms_product_image spu相关图片
pms_product_sale_attr spu属性列表
pms_product_sale_attr_value spu属性列表的值表

pms_sku_info sku，Stock keeping Unit，库存保有单位
pms_sku_image sku相关图片
pms_sku_attr_value sku属性 
pms_sku_sale_attr_value sku属性列表的值表


商品详情页面的数据结构：
大部分是sku相关，一部分是spu相关

1 sku的结构  pms_sku_
2 spu的结构  pms_spu_
3 类目的结构 pms_catalog_
4 属性的结构 pms_attr_


平台属性，这些属性在搜索栏的筛选里面，例如内存16g，鞋码44，长度1米等等
这些属性用来搜索时过滤，每一个商品sku可以拥有很多个这种属性
pms_base_attr_info
pms_base_attr_value     


销售属性列表，给用户选择商品用的
pms_product_sale_attr
pms_product_sale_attr_value
pms_sku_sale_attr_value


平台属性和销售属性的区别：
商品的平台属性属于电商网站后台管理（整个商品平台的维度下的，所有的笔记本都可以有16g内存）
商品的销售属性属于在电商网站上卖商品的商家管理(属于某一件商品的维度下的，同一型号的售卖中的笔记本只会有一种是16内存)



SPU 是商品信息聚合的最小单位。
例如：品牌苹果+型号：5s可以确定一个产品，即SPU
再加上颜色白色，尺码4.0，即表示一个SKU
SPU + 颜色 + 尺码，就是一个SKU，SKU是从属于SPU的关系

SPU由电商平台编辑管理
SKU由电商编辑管理
也就是说，spu是商城用来规范商品的，sku是商家自己决定买什么货物的

spu的范围通常比sku要大


--------------------


架构演变过程

ORM单体架构，前后端所有的模块都放在一起
MVC 分三层
RPC 多个服务，各个模块独立通讯
SOA 多个服务，注册中心管理模块

-----------------------------------------------------
vi查找功能
1、命令模式下输入“/字符串”，例如“/Section 3”。
2、如果查找下一个，按“n”即可。

redis整合：

修改配置文件sudo vim xxx/redis.conf
1.注释#bind 127.0.0.1

2.
修改daemonize yes 初始值为no

关闭保护模式
protected-mode no 设置为no

3.（可选）
配置redis访问密码
requirepass test123


将redis的启动脚本复制一份放到/etc/init.d目录下
cp /usr/local/redis-4.0.6/utils/redis_init_script /etc/init.d/redisd
注意，查看这个启动脚本，需要到对应的配置路径


先切换到/etc/init.d目录下

然后执行自启命令

[root@iZwz991stxdwj560bfmadtZ init.d]# chkconfig redisd on
service redisd does not support chkconfig　
 

看结果是redisd不支持 chkconfig
解决方法：
使用vim编辑redisd文件，在第一行加入如下两行注释，保存退出
# chkconfig:   2345 90 10
# description:  Redis is a persistent key-value database
注释的意思是，redis服务必须在运行级2，3，4，5下被启动或关闭，启动的优先级是90，关闭的优先级是10。


查看redis进程
ps aux | grep redis
结果：
root       1095  0.0  0.2 134176  2416 ?        Rsl  12:47   0:03 /usr/local/bin/redis-server *:6379
root       2029  0.0  0.0 112728   972 pts/0    S+   13:48   0:00 grep --color=auto redis

连接redis
redis-cli -h host -p port -a password
我们这个redis没有设置密码
redis-cli -h 192.168.43.192 -p 6379

查看redis的内容，查看所有的键：
keys *
结果：
1) "sku:100:info"
2) "sku:107:info"
3) "sku:20:info"
4) "user:202cb962ac59075b964b07152d234b70:info"

上面那个user是我们在代码中使用了jedis框架中的
jedis.setex()方法设置进去的用户和密码键对值




缓存穿透
是利用redis和mysql的机制(redis缓存一旦不存在，就访问mysql)，直接绕过缓存访问mysql，而制造的db请求压力
一般在代码中防止该现象的发生
解决：
缓存穿透最好解决，如果redis和mysql中同时没有这个数据，直接在redis中缓存一个空字符，
这样下一次访问数据时，redis会直接取一个空字符给前端


缓存击穿
是某一个热点key在高并发访问的情况下，突然失效，导致大量的并发打进mysql数据库的情况


缓存雪崩
缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，导致的db崩溃
解决：设置不同的缓存失效时间


穿透：利用不存在的key去攻击mysql数据库
雪崩：缓存中的很多key失效，导致数据库负载过重宕机
击穿：在正常的访问情况下，如果缓存失效，如果保护mysql，重启缓存的过程
使用redis数据库的分布式锁，解决mysql的访问压力问题



-----------------------------------------------------

VMware安装centos7

配置ip信息：
192.168.8.198
255.255.255.0
192.168.8.1
192.168.8.1

主机名：vcen

root密码：WRXdevcen7

创建管理员用户：
wrx
密码：
WRXcen777

第二台虚拟机
配置ip信息
192.168.8.239
255.255.255.0
192.168.8.1
192.168.8.1


-----------------

命令：

关机：shutdown -h now
查看ip地址：ip addr

测试虚拟机与本机的网络通信：
(1) 在虚拟机终端ping本机的ip；在本机终端ping虚拟机的ip
(2) 随意第一项的修改ip地址，尝试能否ping通
如果(1)可以ping通，但是(2)失败，就表明双方处于同一局域网内
如果本机和虚拟机的ip地址前三段是相同的，基本也是处于同一局域网内

查看是否安装ssh，如果打印了openssh和libssh那么就是安装了，貌似安装centos7时，默认安装ssh
rpm -qa | grep ssh

rpm -qa | grep ssh 可以看到系统中ssh安装包
ps -ef | grep ssh 查看ssh服务有没有运行,如果有,可以看到类似以下内容:
root 2659 1 0 18:31 ? 00:00:00 /usr/sbin/sshd
root 2702 2618 0 18:38 pts/0 00:00:00 grep ssh
这证明ssh已经在运行了,进程名为sshd 如果没有运行,可以通过以下命令运行之: 
service sshd start


xshell连接centos7，安装lrzsz，用于传输文件
yum install lrzsz -y
检查是否安装成功。
rpm -qa | grep lrzsz
执行，把文件传输到当前centos的目录下
在Windows系统下使用xshell，输入命令时，会自动弹出文件选择框。
rz -y

-----------------

本地下载jdk，解压jdk
tar -xzvf jdk-8u131-linux-x64.tar.gz -C /opt
jdk设置环境变量：
配置环境 vi  /etc/profile 并在最后添加
export JAVA_HOME=/opt/jdk1.8.0_221
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
执行profile文件（通常用于重新执行刚修改的初始化文件，使之立即生效，而不必注销并重新登录）
source /etc/profile


redis解压安装:
tar -zxf redis-3.2.13.tar.gz
安装gcc
yum -y install gcc

安装zip
yum install -y unzip zip
unzip [-cflptuvz][-agCjLMnoqsVX][-P <密码>][.zip文件][文件][-d <目录>][-x <文件>] 或 unzip [-Z]
解压dubbo



解压tomcat之后，配置apache-tomcat-8.5.24/conf/server.xml
<Context path="/dubbo" docBase="/opt/dubbo" debug="0" privileged="true" />
增加一个Context容器，用于dubbo的运行

进入到tomcat的bin目录
cd /home/hstomcat/apache-tomcat-7.0.63/bin
启动tomcat
使用ls命令，可以看到bin目录下的文件

运行bin目录下的启动命令脚本
sh startup.sh或者./startup.sh
这样tomcat就启用了

查看tomcat进程是否启动
ps aux | grep tomcat

关闭tomcat，同样在tomcat的bin目录下
sh shutdown.sh

重启可以理解为先关闭tomcat+再启动tomcat
sh shutdown.sh
sh startup.sh

如果用命令查看到tomcat已经启动，但是无法浏览器无法访问
设置防火墙
firewall-cmd --permanent --zone=public --add-port=8080/tcp  
firewall-cmd --reload  
firewall-cmd --zone=public --query-port=8080/tcp 

查看防火墙状态
firewall-cmd --state
停止firewall
systemctl stop firewalld.service
禁止firewall开机启动
systemctl disable firewalld.service 


dataLogDir=/opt/zookeeper/logs

访问http://192.168.43.192:8080/dubbo，输入默认的用户名和密码（都是root）


查看zookeeper的已连接服务
cd/usr/local/zookeeper-3.4.8/bin
./zkCli.sh -server 127.0.0.1:2181
ls /
或者： ls /dubbo


----------------------------------------------------------

安装图片存储服务器FastDFS
安装gcc和其他运行时库：
yum install gcc-c++ -y
yum -y install zlib zlib-devel pcre pcre-devel gcc gcc-c++ openssl openssl-devel libevent libevent-devel perl unzip net-tools wget
yum -y install libevent
yum install perl*

把libfastcommon上传和安装，安装路径放在/usr/local/下：
rz -y（执行这个命令时会自动弹出文件选择框）
tar -zxvf libfastcommonV1.0.7.tar.gz -C /usr/local/
进行编译安装：
cd /usr/local/libfastcommon-1.0.7/
./make.sh           #编译
./make.sh install   #安装
进入usr/lib64查看生成的so文件，并复制到usr/lib下
cp libfastcommon.so /usr/lib

安装FastDFS_v5.05，上传和解压
rz -y
tar -zxvf FastDFS_v5.05.tar.gz -C /usr/local
/FastDFS/目录下，编译和安装 
cd /usr/local/FastDFS/
./make.sh && ./make.sh install 
复制
cp /usr/local/FastDFS/conf/* /etc/fdfs/
注意：可以不用复制，只需要在开机启动，即etc/init.d/ 目录下，
      将安装时自动生成的两个文件fdfs_storaged和fdfs_trackerd中的CONF=/etc/fdfs/tracker.conf 改为FastDFS/conf/下的对应文件即可

------------------
以下是安装FastDFS_v5.05之后进行的配置

tracker和storage
tracker对storage进行管理和调用，tracker一般会有一个
storage负责对文件的存储，storage可以安装在多个服务器内，形成多个storage一起提供存储服务
所以storage要配置tracker服务器的ip地址，而tracker不需要


进入/etc/fdfs/，修改tracker配置文件
把base_path改成/opt/fastdfs，这个目录如果没有要自行创建
还有其他例如 http.server_port=8080 可以按需要去改。
vim /etc/fdfs/tracker.conf
base_path=/opt/fastdfs


进入/etc/fdfs/，修改storage配置文件
vi /etc/fdfs/storage.conf
修改软件目录路径：
base_path=/opt/fastdfs
Storage存储文件的目录,存放文件的位置(store_path)，如果没有这个目录需要创建 mkdir /opt/fastdfs/fdfs_storage
store_path0=/opt/fastdfs/fdfs_storage 
#如果有多个挂载磁盘则定义多个store_path，如下
#store_path1=.....
#store_path2=.....
配置tracker服务器ip：
tracker_server=192.168.43.192:22122


配置开机自启动：
安装之后已经在etc/init.d/下创建了fdfs_storaged  fdfs_trackerd，但是需要修改部分配置：
vi ../../etc/init.d/fdfs_trackerd
把/usr/local/bin/改为/usr/local/FastDFS/  stop和restart都要改
storaged也要改
vi ../../etc/init.d/fdfs_storaged

将启动脚本加入linux服务（chkconfig开机自启）
chkconfig  --add  fdfs_trackerd
chkconfig  --add  fdfs_storaged

启动服务
service fdfs_trackerd start
service fdfs_storaged start

ps命令过滤查看
ps -aux | grep fdfs


上传测试：
修改/etc/fdfs/client.conf
vi /etc/fdfs/client.conf
base_path=/opt/fastdfs
tracker_server=192.168.43.192:22122


把一张图传到centos上，再上传
rz -y
/usr/bin/fdfs_test  /etc/fdfs/client.conf  upload  ./panda.jpg
上传成功之后会有地址打印出来
此时还不能访问，因为fdfs不是web服务器，不支持http，它只是一个图片存储服务器
需要集成nginx
http://192.168.43.192/group1/M00/00/00/wKgrwF3Q80KAJ28_AABUxbVIpSY739_big.jpg

如果图片上传成功，可以在以下对应的上传路径上查看：
/opt/fastdfs/fdfs_storage/data 
/00/00/wKhDo1qipbiAJC6iAAB1tayPlqs094_big.jpg

-----------------------------

nginx
FastDFS和nginx整合
上传并解压
rz -y
tar -zxvf fastdfs-nginx-module_v1.16.tar.gz -C /usr/local 
解压后把/usr/local/fastdfs-nginx-module/src/config的内容修改一部分，修改config文件将/usr/local/...路径改为/usr/...
/usr/include/fastdfs /usr/include/fastcommon/
复制文件
cp mod_fastdfs.conf /etc/fdfs
vi /etc/fdfs/mod_fastdfs.conf 
修改内容
base_path=/opt/fastdfs
......
tracker_server=192.168.43.192:22122
......
url_have_group_name = true
......
store_path0=/opt/fastdfs/fdfs_storage

将libfdfsclient.so拷贝至/usr/lib下
cp /usr/lib64/libfdfsclient.so /usr/lib/


安装nginx

创建目录
mkdir -p /var/temp/nginx/client
安装pcre库
yum -y install pcre-devel
安装zlib库
yum install -y zlib-devel

上传nginx-1.12.2压缩包到/usr/local目录下，并解压
tar -zxvf nginx-1.12.2.tar.gz -C /usr/local
添加fastdfs-nginx-module模块
cd nginx-1.8.0
./configure \
--prefix=/usr/local/nginx \
--pid-path=/var/run/nginx/nginx.pid \
--lock-path=/var/lock/nginx.lock \
--error-log-path=/var/log/nginx/error.log \
--http-log-path=/var/log/nginx/access.log \
--with-http_gzip_static_module \
--http-client-body-temp-path=/var/temp/nginx/client \
--http-proxy-temp-path=/var/temp/nginx/proxy \
--http-fastcgi-temp-path=/var/temp/nginx/fastcgi \
--http-uwsgi-temp-path=/var/temp/nginx/uwsgi \
--http-scgi-temp-path=/var/temp/nginx/scgi \
--add-module=/usr/local/fastdfs-nginx-module/src

编译
make
安装
make install
编辑nginx.conf：
vi /usr/local/nginx/conf/nginx.conf
#server_name  localhost;
#如果fastFDS在本机，使用localhost也可以
server_name  192.168.43.192;
#charset koi8-r;
#access_log  logs/host.access.log  main;
location / {
	#root   html;
	#index  index.html index.htm;
	ngx_fastdfs_module;
}

运行：
/usr/local/nginx/sbin/nginx
之后就可以在浏览器中查看之前上传的图片了

报错修复：
运行
/usr/local/nginx/sbin/nginx
报错
[root@vcen /]# nginx: [emerg] open() "/var/run/nginx/nginx.pid" failed (2: No such file or directory)
var/run/是临时文件所以直接在其他目录中创建：
创建pid目录：
mkdir /usr/local/nginx/logs
配置文件：
vi /usr/local/nginx/conf/nginx.conf
在原本注释掉的pid下再增加一行
pid        /usr/local/nginx/logs/nginx.pid;


设置开机启动，首先添加一个执行权限给rc.local
chmod  755  /etc/rc.d/rc.local
vi /etc/rc.d/rc.local
加一行
/usr/local/nginx/sbin/nginx

-----------------------------

springboot + fastFDS

在github下载fastFDS，项目中直接clone
E:\MyGmall>git clone https://github.com/happyfish100/fastdfs-client-java
fastdfs-client-java，maven，LifeRecycle中执行
install
执行完成之后，gmall-manage-web的pom里面依赖maven：
<dependency>
	<groupId>org.csource</groupId>
	<artifactId>fastdfs-client-java</artifactId>
	<version>1.27-SNAPSHOT</version>
</dependency>





-----------------------------

常用查询过滤命令：

过滤命令常接 grep 用来过滤，中间用分号隔开，后面接过滤内容：
ps -A | grep xxx  查看全部进程中带有xxx的


查看某个软件是否已安装，带过滤
1、rpm包安装的，可以用rpm -qa看到，如果要查找某软件包是否安装，用 rpm -qa | grep “软件或者包的名字”。
rpm -qa | grep ruby

2、以deb包安装的，可以用dpkg -l能看到。如果是查找指定软件包，用dpkg -l | grep “软件或者包的名字”；
dpkg -l | grep ruby

3、yum方法安装的，可以用yum list installed查找，如果是查找指定包，命令后加 | grep “软件名或者包名”；
yum list installed | grep ruby


ps命令：
ps -aux  查看所有进程，附带有cpu占用信息。也可以加grep命令进行过滤

ps aux | grep tomcat
ps aux | grep zookeeper

-----------------------------------------------------

dubbo包括了注册中心、监控中心在内的多个功能：
Provider: 暴露服务的服务提供方。
Consumer: 调用远程服务的服务消费方。
Registry: 服务注册与发现的注册中心。
Monitor: 统计服务的调用次调和调用时间的监控中心。
Container: 服务运行容器。

调用关系说明
1.服务容器负责启动，加载，运行服务提供者。
2.服务提供者在启动时，向注册中心注册自己提供的服务。
3.服务消费者在启动时，向注册中心订阅自己所需的服务。
4.注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。
5.服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。
6.服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。

这个框架要完成调度必须要有一个分布式的注册中心,存储所有服务的元数据,用到zookeeper。
zookeeper用来注册服务和负载均衡。

Dubbo的Provider，Consumer在启动时都会创建一个注册中心，
注册中心可以选择Zookeeper。支持4种注册中心,（multicast,zookeeper,redis,simple），常用的是Zookeeper。
Dubbo里默认使用zkclient来操作zookeeper服务器，其对zookeeper原始客户单做了一定的封装，
操作zookeeper时能便捷一些，比如不需要手动处理session超时，不需要重复注册watcher等等。

RegistryFactory类是注册中心的入口，调用注册中心方法的类是RegistryProtocol。

maven依赖时dubbo时，要加上zkclient和Zookeeper
 

-----------------------------------------------------

注册中心zookeeper

zookeeper配置：
##ZooKeeper服务器存储快照文件的目录,必须配值,建议放置在var目录下
dataDir=/opt/zookeeper-3.4.11/data

来到bin下，执行
./zkServer.sh start
./zkServer.sh status

查看进程
ps -ef | grep zookeeper

-----------------------------------------------------

设置开机启动

dubbo和zookeeper都可以设置，dubbo主要是设置tomcat(因为tomcat里面配置了启动dubbo)
zookeeper设置其本身

设置开机自启动的两种方法：
1 /etc/rc.local 中配置
2 /etc/init.d 目录下，添加脚本，chmod + r添加脚本权限（或者 chmod 777）
  使用chkconfig --add命令增加该脚本，重启后chkconfig --list查看 （chkconfig就是服务配置管理命令）
  不用重启，也可直接调用：service dubbo-admin start

Linux chkconfig命令用于检查，设置系统的各种服务。
chkconfig [--add][--del][--list][系统服务] 或 chkconfig [--level <等级代号>][系统服务][on/off/reset]
--add　增加所指定的系统服务，让chkconfig指令得以管理它，并同时在系统启动的叙述文件内增加相关数据。
--del　删除所指定的系统服务，不再由chkconfig指令管理，并同时在系统启动的叙述文件内删除相关数据。


-----------------------------------------------------

将user模块拆分为 user-service和user-web，并在dubbo上部署。


-----------------------------------------------------

springboot + dubbo的坑

1. 防火墙要关闭，或者开放对应的端口
2. @service等注解要换成dubbo的注解
3. web微服务的 @Autowired改为@Reference（这个@Reference是dubbo库里面的）

排查问题：
1. 可以用ps命令查看tomcat(里面部署了dubbo)和zookeeper是否启动
2. 可以进入zookeeper的zkCli脚本查看在zookeeper注册的服务
3. 微服务注册成功会有打印： Starting ZkClient event thread.

如果微服务的provider和consumer都注册成功，那么可以在dubbo界面的consumer的详情，里面就有访问的ip address
这个ip地址是和tomcat那台机器的地址不一样的
tomcat可以在虚拟机的ip地址后加8080访问，再加/dubbo可以访问dubbo的管理界面

如果是本地用dubbo部署了微服务，可以直接通过本地ip访问

-----------------------------------------------------

nginx负载均衡
原理：
开发时，idea关闭微服务的单例模式，同一个module启动多次，这样就有多个相同功能的微服务同时对外提供服务，
配置启动nginx，让这多个微服务对应的端口号或者ip不同，这样多个相同功能的微服务同时服务多个客户端，实现负载均衡。
(IntelliJ IDEA默认运行项目后，再点击运行就是重启，但有时候，需要配置项目的不同端口号，同时运行)

idea操作步骤：
取消single instance only（或者勾选allow parallel run），启动一个测试负载均衡的微服务；把之前启动的微服务端口更换，再次启动另一个相同的微服务。
nginx配置时要配置对应的端口号，


-----------------------------------------------------

搜素引擎 elasticSearch ，简称es，其实就是一个数据存储库，NoSQL
新准备一台虚拟机，centos7，添加用户，安装jdk和配置jdk环境变量，

把elasticsearch-6.3.1.tar.gz上传至虚拟机
首先用root用户解压
tar C zxvf elasticsearch-6.3.1.tar.gz
然后用root用户授权
chmod 777 -R elasticsearch-6.3.1

修改elasticsearch/config的文件：
elasticSearch.yml、jvm.Opitons

elasticSearch.yml中配置es的host
network.host:192.168.43.199
http.port:9200

jvm.Opitons中配置es能够使用jvm内存大小
-Xms 和 -Xmx需要配置的相等，不然无法启动成功。
-Xms512m
-Xmx512m


centos
修改系统最大文件数
vi /etc/security/limits.conf
加入：（*代表所有用户）
* soft nofile 65536
* hard nofile 65536
* soft nproc 4096
* hard nproc 4096

vi /etc/sysctl.conf 
加入：
vm.max_map_count=655360
fs.file-max=655360


直接启动可执行文件：
elasticsearch-6.3.1/bin/elasticsearch


启动成功后可以访问，这时会返回一个包含elasticSearch信息的json数据
192.168.43.199:9200
如果访问失败，可以配置防火墙或者直接关闭防火墙
清空data和logs数据
192.168.43.199:9200/_cat/nodes?v

elasticsearch详情信息
http://192.168.43.199:9200/

如果访问上面这个两个链接，可以显示内容，就表示配置成功

-----------------------------

nohup命令启动elasticSearch和kibana
后台运行，把日志输出到本地文件

-----------------------------

kibana

Kibana是一个开源的分析与可视化平台，设计出来用于和Elasticsearch一起使用的。你可以用kibana搜索、查看存放在Elasticsearch中的数据。
Kibana与Elasticsearch的交互方式是各种不同的图表、表格、地图等，直观的展示数据，从而达到高级的数据分析与可视化的目的。
Elasticsearch、Logstash和Kibana这三个技术就是我们常说的ELK技术栈，可以说这三个技术的组合是大数据领域中一个很巧妙的设计。
一种很典型的MVC思想，模型持久层，视图层和控制层。Logstash担任控制层的角色，负责搜集和过滤数据。Elasticsearch担任数据持久层的角色，负责储存数据。

把kibana的安装包上传至虚拟机，并解压

修改配置文件kibana-6.3.1-linux-x86_64/config/kibana.yml
server.host: "0.0.0.0"
elasticsearch.url: "http://192.168.43.199:9200"

直接启动bin下的可执行文件：
kibana-6.3.1-linux-x86_64/bin/kibana

启动后访问：
http://192.168.43.199:5601/
这个界面是一个完整的web的UI页面，可以通过这个界面对elasticsearch进行增删改查

-----------------------------

elasticsearch分词器

无论是内置的分析器（analyzer），还是自定义的分析器（analyzer），都由三种构件块组成的：character filters ， tokenizers ， token filters。
内置的analyzer将这些构建块预先打包到适合不同语言和文本类型的analyzer中。

中文分词器插件: IK分词器, elasticsearch-analysis-ik6

把elasticsearch-analysis-ik6解压到/opt/es/elasticsearch-6.3.1/plugins下，就可以直接使用功能

-----------------------------

elasticsearch集群配置
默认就是集群的，需要简单配置就可以。负载均衡默认实现。

要实现集群，设置config/elasticsearch.yml，目前配置两台虚拟机集群，192.168.43.200和192.168.43.85
cluster.name: aubin-cluster     #必须相同 
# 集群名称（不能重复）
node.name: els1（必须不同）
# 节点名称，仅仅是描述名称，用于在日志中区分（自定义）
#指定了该节点可能成为 master 节点，还可以是数据节点
node.master: true
node.data: true
path.data: /opt/data
# 数据的默认存放路径（自定义）
path.logs: /opt/logs 
# 日志的默认存放路径 
network.host: 192.168.43.200
# 当前节点的IP地址，这个ip地址要是本机的ip地址，
http.port: 9200 
# 对外提供服务的端口
transport.tcp.port: 9300
#9300为集群服务的端口 
discovery.zen.ping.unicast.hosts: ["172.18.68.11", "172.18.68.12","172.18.68.13"] 
# 集群个节点IP地址，也可以使用域名，需要各节点能够解析 
discovery.zen.minimum_master_nodes: 2 
# 为了避免脑裂，集群节点数最少为 半数+1


cerebro
es集群管理工具，elasticsearch的两台机器是安装在centos虚拟机上的，在windows下解压cerebro，
解压后执行bin下的bat文件
启动后访问
http://localhost:9000/
在webUi界面中，输入连接地址即可
http://192.168.43.85:9200

清空data和logs数据
192.168.43.200:9200/_cat/nodes?v


集群分片复制，一条数据是存放在不同分片中的，而且每个分片在不同的机器上都有复制片
这点可以在cerebro工具上查看，甚至可以创建index时选择分片数


-----------------------------

elasticsearch常用命令，可以用这些命令在kibana上进行增删改查
get 查
post 改
put 增
delete 删

几乎所有的命令都是这种格式
PUT/DELETE/POST/GET /{index}/{type}/{id}


显示所有的索引
GET _cat/indices

-----------------------------

新增一个索引，插入索引可以用post和put
PUT /catalog/product/1
{
  "sku": "SP000001",
  "title": "Elasticsearch for Hadoop",
  "description": "Elasticsearch for Hadoop",
  "author": "Vishal Shukla",
  "ISBN": "1785288997",
  "price": 26.99
}


POST /catalog/product
{
  "sku": "SP000003",
  "title": "Mastering Elasticsearch",
  "description": "Mastering Elasticsearch",
  "author": "Bharvi Dixit",
  "price": 54.99
}

put的格式，{}中写入json格式，这个id可以有也可以没有
PUT /{index}/{type}/{id}
{
  "field": "value",
  ...
}


-----------------------------

删除
DELETE /catalog/product/100

-----------------------------
更改
POST /catalog/product/100/_update
{
  "doc": {
    "price" : 100.01
  }
}

POST /catalog/product/100/_update
{
  "doc": {
    "author": "Albert Paro",
    "title": "Elasticsearch 5.0 Cookbook",
    "description": "Elasticsearch 5.0 Cookbook Third Edition",
    "price": "54.99"
  },
  "doc_as_upsert": true
}


POST /catalog/product/1/_update
{
  "script": {
    "source": "ctx._source.price += params.increment",
    "lang": "painless",
    "params": {
      "increment": 20
    }
  }
}

以上命令都可以在kibanan中执行


-----------------------------
查找
GET /catalog/product/100
关于查找
搜素索引时，执行一条即可
GET /{index}/{type}/{id}
如果没有id，可以使用_search，加_search是搜素全部id：
GET /{index}/{type}/_search


-----------------------------

Ik(中英文分词器插件)有两个：
1 ik_smart （简易分词）
2 ik_max_word （尽最大可能分词）

以下是两种不一样的分词，可以在kibana上尝试
GET _analyze
{
  "analyzer": "ik_smart", 
  "text": "我是中国人"
}
GET _analyze
{
  "analyzer": "ik_max_word", 
  "text": "我是中国人"
}

分词结果影响了索引的数量，尽最大可能分词分的索引数量多一些




-----------------------------------------------------

elasticsearch + stringboot开发

开发之前，在application.properties中加入参数配置，如果不加http或者https，编译时会报错（ IllegalArgumentException Illegal character...）
spring.elasticsearch.jest.uris=http://192.168.43.199:9200


增加电商索引数据
PUT gmall0105
{
 "mappings": {
   "PmsSkuInfo":{
     "properties": {
       "id":{
        "type": "keyword",
        "index": true
      },
      "skuName":{
        "type": "text",
        "analyzer": "ik_max_word"
      },
      "skuDesc":{
        "type": "text"
        , "analyzer": "ik_smart"
      },
      "catalog3Id":{
        "type": "keyword"
      },
      "price":{
        "type": "double"
      },
      "skuDefaultImg":{
        "type": "keyword",
        "index": false
      },
      "hotScore":{
        "type": "double"
      },
      "productId":{
        "type": "keyword"
      },
      "skuAttrValueList":{
        "properties": {
          "attrId":{
            "type":"keyword"
          },
          "valueId":{
            "type":"keyword"
          }
        }
      }
     } 
   }
 } 
}

--------------------

代码开发

实例1，创建索引和查找这个索引：
@Autowired
JestClient jestClient;
@Test
public void contextLoads() throws IOException {
	putTest();
}
public void putTest() throws IOException {
	Article article = new Article();
	article.setId(1);
	article.setAuthor("Tom");
	article.setContent("hello world !");
	article.setTitle("今日消息");
	//构建一个索引功能，类型为news
	Index index = new Index.Builder(article).index("jest").type("news").id("100").build();
	try {
		jestClient.execute(index);
		System.out.println("数据索引成功！");
	} catch (IOException e) {
		e.printStackTrace();
	}
}
//Article是一个bean，也可以是json
//添加完成之后可以直接搜素索引：GET /jest/news/_search或者GET /jest/news/100

// 查找
@Test
public void search(){
	//查询表达式
	String json = "{\n" +
			"    \"query\" : {\n" +
			"        \"match\" : {\n" +
			"            \"content\" : \"hello\"\n" +
			"        }\n" +
			"    }\n" +
			"}";
	//构建搜索功能
	Search search = new Search.Builder(json).addIndex("jest").addType("news").build();
	try {
		SearchResult result = jestClient.execute(search);
		System.out.println(result.getJsonString());
	} catch (IOException e) {
		e.printStackTrace();
	}
}

--------------------

实例2，查找：
String dslStr = searchSourceBuilder.toString();
System.err.println(dslStr);
// 用api执行复杂查询
List<PmsSearchSkuInfo> pmsSearchSkuInfos = new ArrayList<>();
Search search = new Search.Builder(dslStr).addIndex("gmall0105").addType("PmsSkuInfo").build();
SearchResult execute = jestClient.execute(search);




--------------------

实例3，修改
public void postBean() {
	Book article = new Book();
	article.setId(1);
	article.setAuthor("Tom");
	article.setContent("hello Tom!");
	article.setTitle("好消息");
	Index update = new Index.Builder(article).index("jest").type("news").id("101").build();
	try {
		JestResult result = jestClient.execute(update);
		System.out.println("数据修改:"+ result.isSucceeded());
	} catch (IOException e) {
		e.printStackTrace();
	}
}

--------------------

实例4，删除
public void deleteBean() {
	Delete delete = new Delete.Builder("101").index("jest").type("news").build();
	JestResult result = null;
	try {
		result = jestClient.execute(delete);
		System.out.println("数据删除:"+ result.isSucceeded());
	} catch (IOException e) {
		e.printStackTrace();
	}
}

--------------------

面包屑功能：
在商城中，一种手机商品有多个筛选条件：
选择一个内存16g之后，16g就是一个面包屑；
再选一个尺寸5英寸之后，5英寸又是一个面包屑。
这样形成一整条筛选条件之后，可以过滤出少许符合条件的手机。

许多网站都有面包屑导航，不一定是 电子商城的搜索筛选才有面包屑功能

这个项目中，elastic search的搜素结果，提供大量 平台属性值 给用户进行选择过滤，达到面包屑功能的效果





------------------------------------------------------------


用户登录认证：

用户认证中心：


SpringBoot整合token、JWT实现登录认证
JSON Web Token（JWT）是目前最流行的跨域身份验证解决方案


-------------------------

以前的用户身份认证：
Internet服务无法与用户身份验证分开。一般过程如下。
1.用户向服务器发送用户名和密码。
2.验证服务器后，相关数据（如用户角色，登录时间等）将保存在当前会话中。
3.服务器向用户返回session_id，session信息都会写入到用户的Cookie。
4.用户的每个后续请求都将通过在Cookie中取出session_id传给服务器。
5.服务器收到session_id并对比之前保存的数据，确认用户的身份。

这种模式最大的问题是，没有分布式架构，无法支持横向扩展。如果使用一个服务器，该模式完全没有问题。
但是，如果它是服务器群集或面向服务的跨域体系结构的话，则需要一个统一的session数据库库来保存会话数据实现共享，
这样负载均衡下的每个服务器才可以正确的验证用户身份。

一种解决方案是听过持久化session数据，写入数据库或文件持久层等。收到请求后，验证服务从持久层请求数据。该解决方案的优点在于架构清晰，
而缺点是架构修改比较费劲，整个服务的验证逻辑层都需要重写，工作量相对较大。而且由于依赖于持久层的数据库或者问题系统，
会有单点风险，如果持久层失败，整个认证体系都会挂掉。

-------------------------

JWT的原则

JWT的原则是在服务器身份验证之后，将生成一个JSON对象并将其发送回用户，如下所示。
{
"UserName": "Chongchong",
"Role": "Admin",
"Expire": "2018-08-08 20:15:56"
}

之后，当用户与服务器通信时，客户在请求中发回JSON对象。服务器仅依赖于这个JSON对象来标识用户。
为了防止用户篡改数据，服务器将在生成对象时添加签名。
服务器不保存任何会话数据，即服务器变为无状态，使其更容易扩展。




JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源。比如用在用户登录上
JWT 最重要的作用就是对 token信息的防伪作用。
JWT的原理， 
一个JWT由三个部分组成：公共部分、私有部分、签名部分。最后由这三者组合进行base64编码得到JWT。
1、公共部分
主要是该JWT的相关配置参数，比如签名的加密算法、格式类型、过期时间等等。
2、私有部分
用户自定义的内容，根据实际需要真正要封装的信息。一般用户名和密码就是封装在这里面。
3、签名部分
根据用户信息+盐值+密钥生成的签名。如果想知道JWT是否是真实的只要把JWT的信息取出来，
加上盐值和服务器中的密钥就可以验证真伪。所以不管由谁保存JWT，只要没有密钥就无法伪造。
4、base64编码，并不是加密，只是把明文信息变成了不可见的字符串。
但是其实只要用一些工具就可以吧base64编码解成明文，所以不要在JWT中放入涉及私密的信息，因为实际上JWT并不是加密信息。

-------------------------

登录代码：



-------------------------



------------------------------------------------------------

分布式事务

多个事务并发执行，例如订单服务提交引起 支付服务、订单服务、库存服务以及其他服务，这些服务并不是同时并发有可能是不同时间并发
当有一个服务没有完成，例如：库存服务的库存不足无法发货，导致库存服务状态回滚，引起其他服务的并发回滚
分布式事务就是为了解决这个问题，通俗一点就是并发事务之间保持一致性，不求同年同月同日生，但求同年同月同日死。

----------------------

消息中间件 apache-activemq
ActiveMQ是一种开源的基于JMS（Java Message Servie）规范的一种消息中间件的实现。

----------------------

延迟队列

在ActiveMQ的配置文件中配置延迟，用于查询支付宝支付成功接口后发现消息

幂等性检查，短时间内的请相同求，返回同一结果。

----------------------
队列消息是点对点、一对一的模式，一个消息的生产者对应一个消息的消费者，一个消费者消费消息之后，其他消费者不会受到消息；
主题消息是发布/订阅、一对多的模式，一个消息的生产者可能对应多个消息的消费者，每一条消息都会发送给所有的消费者；



------------------------------------------------------------

库存系统：
电商订单到 库存，如果存在两个商品仓库（电商的商品当然不可能只放在一个仓库里面，肯定是多仓库存放）
需要拆成两个订单，简称拆单，拆单不只是根据仓库来拆，可以有其他因素，例如：物品贵重，物流，类型不同（手机和海鲜就需要拆单）等等






------------------------------------------------------------

秒杀








---------------

限流

漏铜算法
令牌桶算法











------------------------------------------------------------

docker + k8s（Kubernetes）
云容器自动化部署


1 git 代码版本控制(svn)
2 jenkins 自动打包测试工具(将写好的代码从git上拉下来，然后打成为服务的jar包)
3 glusterfs 分布式文件存储系统，相当于fastdfs
4 docker 把(springboot)微服务作为一个个单独的容器，单独运行
5 Kubernetes解决的就是docker集群以及各处网络访问的阻碍(它还有负载均衡功能，我们用的是nginx+springcloud)
6 etcd+confd+nginx 将nginx的配置实现动态化，在webui页面上就可以配置nginx的反向代理




------------------------------------------------------------


测试流程

本地启动activeMQ

开启以下微服务
cardService、managerService、orderService、userService、orderWeb、passportWeb、payMent

访问
http://192.168.58.1:8017/toTrade
登录信息
test
202cb962ac59075b964b07152d234b70
登录完成之后显示购物车列表
点击提交订单











